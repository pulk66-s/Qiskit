\documentclass{article}
\usepackage{amsmath}
\begin{document}
\title{Quantum Linear Algebra Notation}
\section{Introduction}

This document explain the notations and diverses things about them

\section{Notations}

Here is the list of the differents notations that you can find in this document
\begin{equation}
    \begin{cases}
        z^* & \text{Complex conjugate} \\
        |\psi\rangle & \text{Vector notation named "ket"} \\
        \langle\psi| & \text{Dual vector notation named "bra"} \\
        \langle\psi|\phi\rangle & \text{Inner product} \\
        |\psi\rangle\langle\phi| & \text{Outer product} \\
        |\psi\rangle\otimes|\phi\rangle & \text{Tensor product} \\
        A* & \text{Complex conjugate of the matrix A} \\
        A^T & \text{Transposition of matrix A} \\
        A^\dag = (A^T)^* = (A^*)^T & \text{Hermitian conjugate of matrix A} \\
        \langle\phi|A|\psi\rangle & \text{Inner product between $|\phi\rangle$ and $A|\psi\rangle$} \\
    \end{cases}
\end{equation}

\subsection{Complex conjugate}

\begin{equation}
    \begin{split}
        z & = a + ib \\
        z^* & = a - ib \\
    \end{split}
\end{equation}
With $a$ and $b$ real numbers.
\subsection{Vector notation}

\begin{equation}
    \begin{split}
        |\psi\rangle & = \begin{pmatrix}
            \alpha \\
            \beta \\
        \end{pmatrix} \\
        \langle\psi| & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \\
    \end{split}
\end{equation}
See how $|\psi\rangle$ is in column and $\langle\psi|$ is in line. It's because
$\langle\psi|$ is the dual vector of $|\psi\rangle$.

\subsection{Inner product}

Inner product of two vectors $|\psi\rangle$ and $|\phi\rangle$ is defined as follows:
\begin{equation}
    \begin{split}
        \langle\psi|\phi\rangle & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \begin{pmatrix}
            \gamma \\
            \delta \\
        \end{pmatrix} \\
        \langle\psi|\phi\rangle & = \alpha^*\gamma + \beta^*\delta \\
    \end{split}
\end{equation}

\subsection{Outer product}

Outer product of two vector is defined as follow

\begin{equation}
    \begin{split}
        |\psi\rangle\langle\phi| & = \begin{pmatrix}
            \alpha \\
            \beta \\
        \end{pmatrix} \begin{pmatrix}
            \gamma^* & \delta^* \\
        \end{pmatrix} \\
        & = \begin{pmatrix}
            \alpha\gamma^* & \alpha\delta^* \\
            \beta\gamma^* & \beta\delta^* \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\subsection{Complex conjugate of matrix}

\begin{equation}
    \begin{split}
        A & = \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix} \\
        A^* & = \begin{pmatrix}
            a^* & b^* \\
            c^* & d^* \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\subsection{Transposition of matrix}

\begin{equation}
    \begin{split}
        A & = \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix} \\
        A^T & = \begin{pmatrix}
            a & c \\
            b & d \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\subsection{Hermitian conjugate of matrix}

\begin{equation}
    \begin{split}
        A & = \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix} \\
        A^\dag & = \begin{pmatrix}
            a^* & c^* \\
            b^* & d^* \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\subsection{Inner product between $|\phi\rangle$ and $A|\psi\rangle$}

\begin{equation}
    \begin{split}
        |\phi\rangle & = \begin{pmatrix}
            \alpha \\
            \beta \\
        \end{pmatrix} \\
        |\psi\rangle & = \begin{pmatrix}
            \gamma \\
            \delta \\
        \end{pmatrix} \\
        A & = \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix} \\
        (|\phi\rangle, A|\psi\rangle) & = \langle\phi|A|\psi\rangle \\
        & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix} \begin{pmatrix}
            \gamma \\
            \delta \\
        \end{pmatrix} \\
        & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \begin{pmatrix}
            a\gamma + b\delta \\
            c\gamma + d\delta \\
        \end{pmatrix} \\
        & = \alpha^*(a\gamma + b\delta) + \beta^*(c\gamma + d\delta) \\
        & = \alpha^*a\gamma + \alpha^*b\delta + \beta^*c\gamma + \beta^*d\delta \\
    \end{split}
\end{equation}

Proof that $(|\phi\rangle, A|\psi\rangle) = (A^\dag|\phi\rangle, |\psi\rangle)$

\begin{equation}
    \begin{split}
        (A^\dag|\phi\rangle, |\psi\rangle) & = \langle\phi|A^\dag|\psi\rangle \\
        & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \begin{pmatrix}
            a^* & c^* \\
            b^* & d^* \\
        \end{pmatrix} \begin{pmatrix}
            \gamma \\
            \delta \\
        \end{pmatrix} \\
        & = \begin{pmatrix}
            \alpha^* & \beta^* \\
        \end{pmatrix} \begin{pmatrix}
            a^*\gamma + c^*\delta \\
            b^*\gamma + d^*\delta \\
        \end{pmatrix} \\
        & = \alpha^*a^*\gamma + \alpha^*c^*\delta + \beta^*b^*\gamma + \beta^*d^*\delta \\
        & = \alpha^*a^*\gamma + \beta^*b^*\gamma + \alpha^*c^*\delta + \beta^*d^*\delta \\
        & = \alpha^*(a^*\gamma + b^*\gamma) + \beta^*(c^*\delta + d^*\delta) \\
        & = \alpha^*(a^* + b^*)\gamma + \beta^*(c^* + d^*)\delta \\
        & = \alpha^*(a + b)^*\gamma + \beta^*(c + d)^*\delta \\
        & = \alpha^*(a\gamma + b\gamma)^* + \beta^*(c\delta + d\delta)^* \\
        & = \alpha^*(a\gamma + b\delta)^* + \beta^*(c\gamma + d\delta)^* \\
    \end{split}
\end{equation}

\section{Definition}

\subsection{Spanning}

A span is a set of vector $|v_1\rangle, |v_2\rangle, \dots, |v_n\rangle$ such that any 
vector $|\psi\rangle$ can be written as a linear combination of the $|v_i\rangle$.

It can be written as:
\begin{equation}
    |v\rangle = \sum_i c_i|v_i\rangle
\end{equation}
Where $c_i$ are complex numbers and $|v_i\rangle$ are vectors.

\subsubsection{Example}

for $C^2$ a spanning can be:
\begin{equation}
    |\psi_1\rangle = \begin{pmatrix}
        1 \\
        0 \\
    \end{pmatrix}, |\psi_2\rangle = \begin{pmatrix}
        0 \\
        1 \\
    \end{pmatrix}
\end{equation}
Because any vector $|\psi\rangle = \begin{pmatrix}
    \alpha \\
    \beta \\
\end{pmatrix}$ can be written as a linear combination of $|\psi_1\rangle$ and $|\psi_2\rangle$:
\begin{equation}
    |\psi\rangle = \alpha|\psi_1\rangle + \beta|\psi_2\rangle
\end{equation}

There is other spanning set in $C^2$ like:
\begin{equation}
    |\psi_2\rangle \equiv \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1 \end{pmatrix},
    |\psi_3\rangle \equiv \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1 \end{pmatrix}
\end{equation}

Since the vector $|\psi\rangle = \begin{pmatrix}
    \alpha \\
    \beta \\
\end{pmatrix}$ can be written as a linear combination of $|\psi_2\rangle$ and $|\psi_3\rangle$:
\begin{equation}
    \begin{split}
        |\psi\rangle & = \alpha|\psi_2\rangle + \beta|\psi_3\rangle \\
        & = \alpha\frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1 \end{pmatrix} + \beta\frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1 \end{pmatrix} \\
        & = \frac{1}{\sqrt{2}}\begin{pmatrix}\alpha + \beta \\ \alpha - \beta \end{pmatrix} \\
        \Longrightarrow |\psi\rangle & = \frac{\alpha + \beta}{\sqrt{2}}|\psi_2\rangle + \frac{\alpha - \beta}{\sqrt{2}}|\psi_3\rangle \\
    \end{split}
\end{equation}

\subsubsection{Linearly dependant}

A set of non zero vectors are linearly dependant if there exist a set of complex numbers $a_1, a_2, \dots, a_n$ not all zero such that
\begin{equation}
    a_1|\psi_1\rangle + a_2|\psi_2\rangle + \dots + a_n|\psi_n\rangle = 0
\end{equation}

\subsubsection{Linearly operator}

\begin{equation}
    \begin{split}
        S & = \sum_{i}^{} a_i|v_i\rangle \\
        \Longrightarrow A(S) & = A(\sum_{i}^{} a_i|v_i\rangle) \\
        & = \sum_{i}^{} a_i A|v_i\rangle
    \end{split}
\end{equation}
Where S is a spanning set and A is a linear operator.

There is interesting linear operator like:

Identity:
\begin{equation}
    I|\psi\rangle \equiv |\psi\rangle, \forall |\psi\rangle \in V, \text{V is a vector space}
\end{equation}

Zero Operator:
\begin{equation}
    0|\psi\rangle \equiv 0, \forall |\psi\rangle \in V, \text{V is a vector space}
\end{equation}

\subsection{Matrix and span}

A linear operator can be applied to a vector $V$ and the result is a vector $W$.
\begin{equation}
    A|v_j\rangle = \sum_{i}^{} A_{ij}|w_i\rangle
\end{equation}

\section{Exercices}

\subsection{Show that (1, -1), (1, 2) and (2, 1) are linearly dependant}

A set of vectors $|\psi_1\rangle, |\psi_2\rangle, \dots, |\psi_n\rangle$ is linearly dependant if there exist a set of complex numbers $c_1, c_2, \dots, c_n$ not all zero such that
\begin{equation}
    c_1|\psi_1\rangle + c_2|\psi_2\rangle + \dots + c_n|\psi_n\rangle = 0
\end{equation}

\begin{equation}
    \begin{split}
        |\psi\rangle & = \begin{pmatrix}
            1 \\
            -1 \\
        \end{pmatrix} \\
        |\phi\rangle & = \begin{pmatrix}
            1 \\
            2 \\
        \end{pmatrix} \\
        |\chi\rangle & = \begin{pmatrix}
            2 \\
            1 \\
        \end{pmatrix} \\
        \Longrightarrow |\psi\rangle + |\phi\rangle - 2|\chi\rangle & = \begin{pmatrix}
            1 \\
            -1 \\
        \end{pmatrix} + \begin{pmatrix}
            1 \\
            2 \\
        \end{pmatrix} - \begin{pmatrix}
            2 \\
            1 \\
        \end{pmatrix} \\
        & = 0
    \end{split}
\end{equation}

\subsection{Exercice 2.2 (Matrix representations: example)}

Suppose $V$ is a vector space with basis vector $|0\rangle$ and $|1\rangle$.
A is a linear operator from $V$ to $V$ such $A|0\rangle = |1\rangle$ and 
$A|1\rangle = |0\rangle$. Give the matrix representation of A in the basis
$|0\rangle$ and $|1\rangle$.

\subsubsection{Theory}

\begin{equation}
    \begin{split}
        A|0\rangle & = |1\rangle \\
        A|1\rangle & = |0\rangle \\
        A & = \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\subsubsection{Verification}

\begin{equation}
    \begin{split}
        A & = \begin{pmatrix}
            0 & 1 \\ 1 & 0 \\
        \end{pmatrix} \\
        |0\rangle & = \begin{pmatrix}
            1 \\ 0 \\
        \end{pmatrix} \\
        |1\rangle & = \begin{pmatrix}
            0 \\ 1 \\
        \end{pmatrix} \\
        \Longrightarrow A|0\rangle & = \begin{pmatrix}
            0 & 1 \\ 1 & 0 \\
        \end{pmatrix} \begin{pmatrix}
            1 \\ 0 \\
        \end{pmatrix} = \begin{pmatrix}
            0 \\ 1 \\
        \end{pmatrix} = |1\rangle \\
        \Longrightarrow A|1\rangle & = \begin{pmatrix}
            0 & 1 \\ 1 & 0 \\
        \end{pmatrix} \begin{pmatrix}
            0 \\ 1 \\
        \end{pmatrix} = \begin{pmatrix}
            1 \\ 0 \\
        \end{pmatrix} = |0\rangle \\
    \end{split}
\end{equation}

\subsection{2.3: (Matrix representation for operator products)}

Suppose $A$ is a linear operator from vector space $V$ to $W$ and $B$ is a linear
operator from $W$ to $X$. Let $|v_i\rangle$, $|w_j\rangle$ and $|x_k\rangle$ be
bases for the vector spaces $V$, $W$, and $X$, respectively. Show that the matrix 
representation for the linear $BA$ is the matrix product of the matrix representations for $B$ and $A$,
with respects to the appropriate bases.

\begin{equation}
    \begin{split}
        |v\rangle & \equiv \alpha|v_i\rangle,
        |w\rangle \equiv \beta|w_j\rangle,
        |x\rangle \equiv \gamma|x_k\rangle \\
        A & = \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{pmatrix} = \sum_{i}^{} \sum_{j}^{} a_{ij} \\
        B & = \begin{pmatrix}
            b_{11} & b_{12} & \dots & b_{1n} \\
            b_{21} & b_{22} & \dots & b_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            b_{m1} & b_{m2} & \dots & b_{mn} \\
        \end{pmatrix} = \sum_{j}^{} \sum_{k}^{} b_{jk} \\
        A|v\rangle & \equiv \alpha A|v_i\rangle \\
        B|w\rangle & \equiv \beta B|w_j\rangle \\
        A|v_i\rangle & = \sum_{j}^{} a_{ij}|w_j\rangle \\
        B|w_j\rangle & = \sum_{k}^{} b_{jk}|x_k\rangle \\
        BA|v_i\rangle & = B(\sum_{j}^{} a_{ij}|w_j\rangle) \\
        & = \sum_{j}^{} a_{ij}B|w_j\rangle \\
        & = \sum_{j}^{} a_{ij}(\sum_{k}^{} b_{jk}|x_k\rangle) \\
        & = \sum_{j}^{} \sum_{k}^{} a_{ij}b_{jk}|x_k\rangle \\
        & = \sum_{k}^{} (\sum_{j}^{} a_{ij}b_{jk})|x_k\rangle \\
        & = \sum_{k}^{} (AB)_{ik}|x_k\rangle \\
        \Longrightarrow BA & = \sum_{i}^{} \sum_{k}^{} (AB)_{ik} \\
    \end{split}
\end{equation}


\subsection{Exercice 2.4}

Show that the identity operator on a vector space V has a matrix representation which is
one along the diagonal and zero everywhere else, if the matrix representation of is taken with respect to the
same input and output bases. This matrix is known as identity matrix

\begin{equation}
    \begin{split}
        I|0\rangle & = |0\rangle \\
        I|1\rangle & = |1\rangle \\
        I & = \begin{pmatrix}
            1 & 0 \\
            0 & 1 \\
        \end{pmatrix} \\
    \end{split}
\end{equation}

\end{document}
